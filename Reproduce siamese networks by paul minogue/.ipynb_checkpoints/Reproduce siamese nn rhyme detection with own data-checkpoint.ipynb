{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/minoguep/rhyme_detection and https://paulminogue.com/index.php/2021/02/14/using-a-siamese-neural-network-to-create-a-simple-rhyme-detector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_567164/3384319969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Subtract\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TerminateOnNaN, CSVLogger\n",
    "tqdm.pandas()\n",
    "\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create dataset\n",
    "We want equally many positive and negative samples of rhyme pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "\n",
    "positive = pd.read_csv(\"../norwegian_rhyme_scheme_corpus/annotation_tool/rhyme_pairs.tsv\", sep=\"\\t\", names=[\"word_a\", \"word_b\"], header=0)\n",
    "negative = pd.read_csv(\"../norwegian_rhyme_scheme_corpus/annotation_tool/negative_rhyme_pairs.tsv\", sep=\"\\t\", names=[\"word_a\", \"word_b\"], header=0)\n",
    "negative = negative.sample(n=len(positive), random_state=seed)\n",
    "positive[\"rhyme\"] = [1]*len(positive)\n",
    "negative[\"rhyme\"] = [0]*len(negative)\n",
    "\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_a</th>\n",
       "      <th>word_b</th>\n",
       "      <th>rhyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huse</td>\n",
       "      <td>bruse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halen</td>\n",
       "      <td>pralen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oksepar</td>\n",
       "      <td>svar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bevare</td>\n",
       "      <td>bare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ly</td>\n",
       "      <td>våbengny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>støy</td>\n",
       "      <td>brast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20575</th>\n",
       "      <td>meier</td>\n",
       "      <td>kne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>synger</td>\n",
       "      <td>land</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16537</th>\n",
       "      <td>frukt</td>\n",
       "      <td>sten</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>rett</td>\n",
       "      <td>brente</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_a    word_b  rhyme\n",
       "0         huse     bruse      1\n",
       "1        halen    pralen      1\n",
       "2      oksepar      svar      1\n",
       "3       bevare      bare      1\n",
       "4           ly  våbengny      1\n",
       "...        ...       ...    ...\n",
       "1757      støy     brast      0\n",
       "20575    meier       kne      0\n",
       "2632    synger      land      0\n",
       "16537    frukt      sten      0\n",
       "2760      rett    brente      0\n",
       "\n",
       "[14506 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create model\n",
    "Copy paste from Pauls notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inputs(phrase_a, phrase_b, tokenizer):\n",
    "    tokenized_phrases = tokenizer.texts_to_sequences([phrase_a, phrase_b])\n",
    "\n",
    "    # now loop through inputs and pad or reduce size if required\n",
    "    tokenized_phrases_for_output = []\n",
    "    for phrase in tokenized_phrases:\n",
    "        if len(phrase) < MAX_LEN:\n",
    "            length_to_pad = MAX_LEN - len(phrase)\n",
    "            phrase_for_output = ([0] * length_to_pad) + phrase\n",
    "        elif len(phrase) > MAX_LEN:\n",
    "            phrase_for_output = phrase[-MAX_LEN:]\n",
    "        else:\n",
    "            phrase_for_output = phrase\n",
    "        tokenized_phrases_for_output.append(phrase_for_output)\n",
    "\n",
    "    return tf.constant(tokenized_phrases_for_output, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  \n",
    "    word_a_input_tokens = Input(\n",
    "      shape=(MAX_LEN, 1), name='word_a_input_tokens'\n",
    "      )\n",
    "    word_b_input_tokens = Input(\n",
    "      shape=(MAX_LEN, 1), name='word_b_input_tokens'\n",
    "      )\n",
    "    \n",
    "    # This is the siamese portion of the model \n",
    "    common_lstm = LSTM(64, return_sequences=False, activation=\"relu\", name=\"common_lstm_layer\")\n",
    "\n",
    "    word_a_lstm_output = common_lstm(word_a_input_tokens)\n",
    "    word_b_lstm_output = common_lstm(word_b_input_tokens)\n",
    "\n",
    "    #concatenate_lstm_outputs\n",
    "    concat_layer = Subtract(name=\"concatenate_lstm_outputs\")(\n",
    "      [word_a_lstm_output, word_b_lstm_output]\n",
    "      )\n",
    "    \n",
    "    # dense layers before final classification\n",
    "    dense_layers = Dense(64, activation=\"relu\", name=\"first_dense_layer\")(concat_layer)\n",
    "    dense_layers = Dropout(0.5)(dense_layers)\n",
    "\n",
    "    dense_layers = Dense(32, activation=\"relu\", name=\"second_dense_layer\")(dense_layers)\n",
    "    dense_layers = Dropout(0.5)(dense_layers)\n",
    "\n",
    "    dense_layers = Dense(8, activation=\"relu\", name=\"third_dense_layer\")(dense_layers)\n",
    "    dense_layers = Dropout(0.5)(dense_layers)\n",
    "\n",
    "    classification_layer = Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(dense_layers)\n",
    "    \n",
    "    model = Model(\n",
    "      inputs=[word_a_input_tokens, word_b_input_tokens], \n",
    "      outputs = classification_layer\n",
    "      )\n",
    "\n",
    "    model.compile(\n",
    "      loss=\"binary_crossentropy\",\n",
    "      metrics=[\"accuracy\"],\n",
    "      optimizer=\"Adam\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7677e1019247bdb458f0625cc1eb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=True, lower=True)\n",
    "tokenizer.fit_on_texts(df['word_a'] + df['word_b'])\n",
    "\n",
    "df['word_tokens'] = df.progress_apply(\n",
    "    lambda row: tokenize_inputs(row['word_a'], row['word_b'], tokenizer), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df['word_tokens']), list(df['rhyme']), stratify=df['rhyme'], \n",
    "    test_size=0.4, random_state=seed\n",
    "    )\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, stratify=y_test, test_size=0.25, random_state=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(X_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "X_val = tf.convert_to_tensor(X_val)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68/68 [==============================] - 5s 45ms/step - loss: 0.6660 - accuracy: 0.6169 - val_loss: 0.5956 - val_accuracy: 0.6692\n",
      "Epoch 2/100\n",
      " 3/68 [>.............................] - ETA: 2s - loss: 0.6936 - accuracy: 0.6380"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tita/.local/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 3s 41ms/step - loss: 0.5894 - accuracy: 0.7663 - val_loss: 0.5030 - val_accuracy: 0.8573\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.5156 - accuracy: 0.8142 - val_loss: 0.4244 - val_accuracy: 0.8663\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.4615 - accuracy: 0.8301 - val_loss: 0.3669 - val_accuracy: 0.8718\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.4137 - accuracy: 0.8459 - val_loss: 0.3383 - val_accuracy: 0.8835\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.4008 - accuracy: 0.8414 - val_loss: 0.3088 - val_accuracy: 0.8932\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.3716 - accuracy: 0.8555 - val_loss: 0.2916 - val_accuracy: 0.8973\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.3490 - accuracy: 0.8644 - val_loss: 0.2789 - val_accuracy: 0.8973\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 3s 48ms/step - loss: 0.3472 - accuracy: 0.8614 - val_loss: 0.2754 - val_accuracy: 0.8946\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.3341 - accuracy: 0.8672 - val_loss: 0.2667 - val_accuracy: 0.9001\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.3319 - accuracy: 0.8690 - val_loss: 0.2636 - val_accuracy: 0.8966\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.3231 - accuracy: 0.8687 - val_loss: 0.2554 - val_accuracy: 0.9001\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.3129 - accuracy: 0.8745 - val_loss: 0.2558 - val_accuracy: 0.8925\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.3085 - accuracy: 0.8730 - val_loss: 0.2518 - val_accuracy: 0.8932\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2997 - accuracy: 0.8773 - val_loss: 0.2454 - val_accuracy: 0.9021\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.3106 - accuracy: 0.8746 - val_loss: 0.2411 - val_accuracy: 0.8966\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.3014 - accuracy: 0.8787 - val_loss: 0.2445 - val_accuracy: 0.8973\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2970 - accuracy: 0.8834 - val_loss: 0.2392 - val_accuracy: 0.8966\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2889 - accuracy: 0.8839 - val_loss: 0.2395 - val_accuracy: 0.8966\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2834 - accuracy: 0.8850 - val_loss: 0.2356 - val_accuracy: 0.9028\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2879 - accuracy: 0.8869 - val_loss: 0.2319 - val_accuracy: 0.9028\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2806 - accuracy: 0.8908 - val_loss: 0.2364 - val_accuracy: 0.9063\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2788 - accuracy: 0.8891 - val_loss: 0.2349 - val_accuracy: 0.9014\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2745 - accuracy: 0.8862 - val_loss: 0.2339 - val_accuracy: 0.8966\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2721 - accuracy: 0.8877 - val_loss: 0.2284 - val_accuracy: 0.9056\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2689 - accuracy: 0.8895 - val_loss: 0.2220 - val_accuracy: 0.9090\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 3s 48ms/step - loss: 0.2676 - accuracy: 0.8910 - val_loss: 0.2309 - val_accuracy: 0.9070\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2718 - accuracy: 0.8879 - val_loss: 0.2287 - val_accuracy: 0.9063\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2677 - accuracy: 0.8890 - val_loss: 0.2319 - val_accuracy: 0.9042\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2628 - accuracy: 0.8934 - val_loss: 0.2286 - val_accuracy: 0.9021\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2615 - accuracy: 0.8895 - val_loss: 0.2251 - val_accuracy: 0.9070\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2645 - accuracy: 0.8888 - val_loss: 0.2231 - val_accuracy: 0.9070\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2602 - accuracy: 0.8925 - val_loss: 0.2201 - val_accuracy: 0.9056\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2633 - accuracy: 0.8914 - val_loss: 0.2236 - val_accuracy: 0.9118\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2506 - accuracy: 0.8960 - val_loss: 0.2163 - val_accuracy: 0.9097\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2535 - accuracy: 0.8930 - val_loss: 0.2225 - val_accuracy: 0.9049\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2559 - accuracy: 0.8956 - val_loss: 0.2148 - val_accuracy: 0.9063\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2507 - accuracy: 0.8973 - val_loss: 0.2184 - val_accuracy: 0.9049\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2543 - accuracy: 0.8979 - val_loss: 0.2163 - val_accuracy: 0.9097\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 3s 47ms/step - loss: 0.2580 - accuracy: 0.8965 - val_loss: 0.2190 - val_accuracy: 0.9090\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2455 - accuracy: 0.8975 - val_loss: 0.2162 - val_accuracy: 0.9076\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 3s 47ms/step - loss: 0.2368 - accuracy: 0.8956 - val_loss: 0.2149 - val_accuracy: 0.9097\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2461 - accuracy: 0.8965 - val_loss: 0.2106 - val_accuracy: 0.9139\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2447 - accuracy: 0.8970 - val_loss: 0.2127 - val_accuracy: 0.9090\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2445 - accuracy: 0.8968 - val_loss: 0.2045 - val_accuracy: 0.9159\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2375 - accuracy: 0.9026 - val_loss: 0.2027 - val_accuracy: 0.9180\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 3s 40ms/step - loss: 0.2389 - accuracy: 0.9014 - val_loss: 0.2186 - val_accuracy: 0.9159\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2273 - accuracy: 0.9036 - val_loss: 0.2263 - val_accuracy: 0.9132\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2396 - accuracy: 0.9028 - val_loss: 0.2210 - val_accuracy: 0.9111\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2310 - accuracy: 0.9035 - val_loss: 0.2054 - val_accuracy: 0.9173\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2417 - accuracy: 0.9013 - val_loss: 0.2027 - val_accuracy: 0.9145\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 3s 48ms/step - loss: 0.2370 - accuracy: 0.9038 - val_loss: 0.2007 - val_accuracy: 0.9132\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2246 - accuracy: 0.9030 - val_loss: 0.2069 - val_accuracy: 0.9152\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2277 - accuracy: 0.9031 - val_loss: 0.1975 - val_accuracy: 0.9152\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2358 - accuracy: 0.8987 - val_loss: 0.2069 - val_accuracy: 0.9166\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2328 - accuracy: 0.9047 - val_loss: 0.2066 - val_accuracy: 0.9207\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2313 - accuracy: 0.9005 - val_loss: 0.2058 - val_accuracy: 0.9194\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2336 - accuracy: 0.9029 - val_loss: 0.2035 - val_accuracy: 0.9214\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 3s 47ms/step - loss: 0.2177 - accuracy: 0.9045 - val_loss: 0.2061 - val_accuracy: 0.9166\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 3s 47ms/step - loss: 0.2284 - accuracy: 0.9015 - val_loss: 0.2015 - val_accuracy: 0.9194\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2228 - accuracy: 0.9045 - val_loss: 0.2021 - val_accuracy: 0.9173\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2222 - accuracy: 0.9084 - val_loss: 0.2138 - val_accuracy: 0.9159\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 3s 49ms/step - loss: 0.2144 - accuracy: 0.9066 - val_loss: 0.2023 - val_accuracy: 0.9194\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2223 - accuracy: 0.9100 - val_loss: 0.2034 - val_accuracy: 0.9173\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2209 - accuracy: 0.9047 - val_loss: 0.1932 - val_accuracy: 0.9242\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2201 - accuracy: 0.9061 - val_loss: 0.1996 - val_accuracy: 0.9235\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2144 - accuracy: 0.9114 - val_loss: 0.2011 - val_accuracy: 0.9194\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2306 - accuracy: 0.9054 - val_loss: 0.1898 - val_accuracy: 0.9263\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2138 - accuracy: 0.9087 - val_loss: 0.2023 - val_accuracy: 0.9256\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2194 - accuracy: 0.9115 - val_loss: 0.1987 - val_accuracy: 0.9256\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2151 - accuracy: 0.9104 - val_loss: 0.1938 - val_accuracy: 0.9269\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2073 - accuracy: 0.9110 - val_loss: 0.2028 - val_accuracy: 0.9318\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2231 - accuracy: 0.9112 - val_loss: 0.1956 - val_accuracy: 0.9256\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2223 - accuracy: 0.9096 - val_loss: 0.1917 - val_accuracy: 0.9290\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2183 - accuracy: 0.9103 - val_loss: 0.1977 - val_accuracy: 0.9290\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 0.2065 - accuracy: 0.9137 - val_loss: 0.1936 - val_accuracy: 0.9290\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2075 - accuracy: 0.9120 - val_loss: 0.2032 - val_accuracy: 0.9249\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2134 - accuracy: 0.9152 - val_loss: 0.1992 - val_accuracy: 0.9359\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 3s 47ms/step - loss: 0.2158 - accuracy: 0.9082 - val_loss: 0.1973 - val_accuracy: 0.9249\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.2022 - accuracy: 0.9145 - val_loss: 0.1901 - val_accuracy: 0.9338\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2079 - accuracy: 0.9142 - val_loss: 0.1871 - val_accuracy: 0.9311\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2125 - accuracy: 0.9192 - val_loss: 0.2061 - val_accuracy: 0.9276\n",
      "Epoch 83/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2043 - accuracy: 0.9162 - val_loss: 0.1856 - val_accuracy: 0.9359\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2026 - accuracy: 0.9175 - val_loss: 0.1897 - val_accuracy: 0.9387\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.1991 - accuracy: 0.9207 - val_loss: 0.1901 - val_accuracy: 0.9366\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.1967 - accuracy: 0.9182 - val_loss: 0.1935 - val_accuracy: 0.9325\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 0.1983 - accuracy: 0.9185 - val_loss: 0.1863 - val_accuracy: 0.9331\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2097 - accuracy: 0.9189 - val_loss: 0.1940 - val_accuracy: 0.9345\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.1957 - accuracy: 0.9201 - val_loss: 0.1926 - val_accuracy: 0.9345\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 3s 45ms/step - loss: 0.2013 - accuracy: 0.9193 - val_loss: 0.1984 - val_accuracy: 0.9338\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.1993 - accuracy: 0.9144 - val_loss: 0.1895 - val_accuracy: 0.9366\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.2035 - accuracy: 0.9164 - val_loss: 0.1905 - val_accuracy: 0.9352\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2015 - accuracy: 0.9159 - val_loss: 0.1835 - val_accuracy: 0.9394\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.1996 - accuracy: 0.9182 - val_loss: 0.1840 - val_accuracy: 0.9345\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.2078 - accuracy: 0.9167 - val_loss: 0.1883 - val_accuracy: 0.9283\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.2065 - accuracy: 0.9162 - val_loss: 0.1839 - val_accuracy: 0.9345\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 0.1908 - accuracy: 0.9234 - val_loss: 0.2040 - val_accuracy: 0.9290\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 3s 46ms/step - loss: 0.1991 - accuracy: 0.9182 - val_loss: 0.1832 - val_accuracy: 0.9400\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 3s 44ms/step - loss: 0.1956 - accuracy: 0.9172 - val_loss: 0.1930 - val_accuracy: 0.9359\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 4s 55ms/step - loss: 0.1954 - accuracy: 0.9220 - val_loss: 0.1830 - val_accuracy: 0.9387\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\"models/rhyme_model.hdf5\",monitor=\"val_loss\")\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]],\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint, terminate_on_nan, csv_logger],\n",
    "    validation_data=([X_val[:, 0], X_val[:, 1]], y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model(\"models/rhyme_model.hdf5\")\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "y_pred = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      2176\n",
      "           1       0.97      0.90      0.93      2176\n",
      "\n",
      "    accuracy                           0.94      4352\n",
      "   macro avg       0.94      0.94      0.94      4352\n",
      "weighted avg       0.94      0.94      0.94      4352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric 1: Du sitter og ser så dum ut\n",
      "Lyric 2: Idiot, din jævla stut\n",
      "Rhyme(0.9848999977111816)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Er du sikker på at dette er med vilje?\n",
      "Lyric 2: Hjertet ditt smaker vanilje\n",
      "Rhyme(0.9991000294685364)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Dette vokser\n",
      "Lyric 2: Satans underbukser\n",
      "Rhyme(0.9991000294685364)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Hva er meninga med livet?\n",
      "Lyric 2: Ikke vet jeg.\n",
      "Non-rhyme(0.03720000013709068)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Dette sier Gud\n",
      "Lyric 2: Ve deg! lille menneske\n",
      "Non-rhyme(0.0005000000237487257)\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "   [\"Du sitter og ser så dum ut\", \"Idiot, din jævla stut\"], \n",
    "   [\"Er du sikker på at dette er med vilje?\", \"Hjertet ditt smaker vanilje\"], \n",
    "   [\"Dette vokser\", \"Satans underbukser\"],\n",
    "   [\"Hva er meninga med livet?\", \"Ikke vet jeg.\"], \n",
    "   [\"Dette sier Gud\", \"Ve deg! lille menneske\"], \n",
    "]\n",
    "\n",
    "sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "for i in range(len(samples)):\n",
    "    print(f\"Lyric 1: {samples[i][0]}\")\n",
    "    print(f\"Lyric 2: {samples[i][1]}\")\n",
    "    print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try again with different 2:3 positive to negative ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "\n",
    "positive = pd.read_csv(\"../norwegian_rhyme_scheme_corpus/annotation_tool/rhyme_pairs.tsv\", sep=\"\\t\", names=[\"word_a\", \"word_b\"], header=0)\n",
    "negative = pd.read_csv(\"../norwegian_rhyme_scheme_corpus/annotation_tool/negative_rhyme_pairs.tsv\", sep=\"\\t\", names=[\"word_a\", \"word_b\"], header=0)\n",
    "negative = negative.sample(n=len(positive)*2, random_state=seed)\n",
    "positive[\"rhyme\"] = [1]*len(positive)\n",
    "negative[\"rhyme\"] = [0]*len(negative)\n",
    "\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_a</th>\n",
       "      <th>word_b</th>\n",
       "      <th>rhyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huse</td>\n",
       "      <td>bruse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halen</td>\n",
       "      <td>pralen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oksepar</td>\n",
       "      <td>svar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bevare</td>\n",
       "      <td>bare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ly</td>\n",
       "      <td>våbengny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21190</th>\n",
       "      <td>tapen</td>\n",
       "      <td>øye</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14441</th>\n",
       "      <td>tilende</td>\n",
       "      <td>strålefjed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>takten</td>\n",
       "      <td>tusen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20767</th>\n",
       "      <td>mindre</td>\n",
       "      <td>ve</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11080</th>\n",
       "      <td>høye</td>\n",
       "      <td>lammet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21759 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_a      word_b  rhyme\n",
       "0         huse       bruse      1\n",
       "1        halen      pralen      1\n",
       "2      oksepar        svar      1\n",
       "3       bevare        bare      1\n",
       "4           ly    våbengny      1\n",
       "...        ...         ...    ...\n",
       "21190    tapen         øye      0\n",
       "14441  tilende  strålefjed      0\n",
       "4952    takten       tusen      0\n",
       "20767   mindre          ve      0\n",
       "11080     høye      lammet      0\n",
       "\n",
       "[21759 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39df8da740544ed5b7cb411c4a1245f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#copy paste from above cells\n",
    "tokenizer = Tokenizer(char_level=True, lower=True)\n",
    "tokenizer.fit_on_texts(df['word_a'] + df['word_b'])\n",
    "\n",
    "df['word_tokens'] = df.progress_apply(\n",
    "    lambda row: tokenize_inputs(row['word_a'], row['word_b'], tokenizer), axis=1\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df['word_tokens']), list(df['rhyme']), stratify=df['rhyme'], \n",
    "    test_size=0.4, random_state=seed\n",
    "    )\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, stratify=y_test, test_size=0.25, random_state=seed\n",
    "    )\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "X_val = tf.convert_to_tensor(X_val)\n",
    "y_val = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 [==============================] - 6s 44ms/step - loss: 0.5733 - accuracy: 0.6466 - val_loss: 0.4560 - val_accuracy: 0.6668\n",
      "Epoch 2/100\n",
      "  3/102 [..............................] - ETA: 4s - loss: 0.4896 - accuracy: 0.6406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tita/.local/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 4s 43ms/step - loss: 0.4691 - accuracy: 0.7550 - val_loss: 0.4203 - val_accuracy: 0.8948\n",
      "Epoch 3/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.4409 - accuracy: 0.8653 - val_loss: 0.3935 - val_accuracy: 0.9104\n",
      "Epoch 4/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.4264 - accuracy: 0.8808 - val_loss: 0.3612 - val_accuracy: 0.9127\n",
      "Epoch 5/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.4013 - accuracy: 0.8818 - val_loss: 0.3436 - val_accuracy: 0.9182\n",
      "Epoch 6/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.3970 - accuracy: 0.8830 - val_loss: 0.3353 - val_accuracy: 0.9145\n",
      "Epoch 7/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.3771 - accuracy: 0.8817 - val_loss: 0.3196 - val_accuracy: 0.9219\n",
      "Epoch 8/100\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 0.3611 - accuracy: 0.8870 - val_loss: 0.3025 - val_accuracy: 0.9242\n",
      "Epoch 9/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.3645 - accuracy: 0.8846 - val_loss: 0.3060 - val_accuracy: 0.9214\n",
      "Epoch 10/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.3601 - accuracy: 0.8817 - val_loss: 0.3006 - val_accuracy: 0.9182\n",
      "Epoch 11/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.3471 - accuracy: 0.8844 - val_loss: 0.2764 - val_accuracy: 0.9251\n",
      "Epoch 12/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.3341 - accuracy: 0.8877 - val_loss: 0.2777 - val_accuracy: 0.9228\n",
      "Epoch 13/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.3342 - accuracy: 0.8890 - val_loss: 0.2672 - val_accuracy: 0.9283\n",
      "Epoch 14/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.3300 - accuracy: 0.8869 - val_loss: 0.2686 - val_accuracy: 0.9251\n",
      "Epoch 15/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.3300 - accuracy: 0.8896 - val_loss: 0.2684 - val_accuracy: 0.9265\n",
      "Epoch 16/100\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 0.3257 - accuracy: 0.8915 - val_loss: 0.2621 - val_accuracy: 0.9210\n",
      "Epoch 17/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.3166 - accuracy: 0.8875 - val_loss: 0.2566 - val_accuracy: 0.9292\n",
      "Epoch 18/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.3110 - accuracy: 0.8930 - val_loss: 0.2545 - val_accuracy: 0.9233\n",
      "Epoch 19/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.3115 - accuracy: 0.8926 - val_loss: 0.2509 - val_accuracy: 0.9338\n",
      "Epoch 20/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.3081 - accuracy: 0.8974 - val_loss: 0.2465 - val_accuracy: 0.9301\n",
      "Epoch 21/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2925 - accuracy: 0.9007 - val_loss: 0.2371 - val_accuracy: 0.9334\n",
      "Epoch 22/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2987 - accuracy: 0.9007 - val_loss: 0.2351 - val_accuracy: 0.9380\n",
      "Epoch 23/100\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 0.2910 - accuracy: 0.9023 - val_loss: 0.2386 - val_accuracy: 0.9315\n",
      "Epoch 24/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2893 - accuracy: 0.9026 - val_loss: 0.2356 - val_accuracy: 0.9343\n",
      "Epoch 25/100\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 0.2895 - accuracy: 0.9022 - val_loss: 0.2297 - val_accuracy: 0.9306\n",
      "Epoch 26/100\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 0.2830 - accuracy: 0.9045 - val_loss: 0.2236 - val_accuracy: 0.9393\n",
      "Epoch 27/100\n",
      "102/102 [==============================] - 4s 40ms/step - loss: 0.2850 - accuracy: 0.8999 - val_loss: 0.2223 - val_accuracy: 0.9384\n",
      "Epoch 28/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.2893 - accuracy: 0.9009 - val_loss: 0.2247 - val_accuracy: 0.9265\n",
      "Epoch 29/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2794 - accuracy: 0.9059 - val_loss: 0.2161 - val_accuracy: 0.9389\n",
      "Epoch 30/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2752 - accuracy: 0.9082 - val_loss: 0.2178 - val_accuracy: 0.9398\n",
      "Epoch 31/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2729 - accuracy: 0.9101 - val_loss: 0.2128 - val_accuracy: 0.9398\n",
      "Epoch 32/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2698 - accuracy: 0.9103 - val_loss: 0.2105 - val_accuracy: 0.9426\n",
      "Epoch 33/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2652 - accuracy: 0.9087 - val_loss: 0.2255 - val_accuracy: 0.9389\n",
      "Epoch 34/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.2578 - accuracy: 0.9147 - val_loss: 0.2181 - val_accuracy: 0.9403\n",
      "Epoch 35/100\n",
      "102/102 [==============================] - 4s 40ms/step - loss: 0.2686 - accuracy: 0.9115 - val_loss: 0.2164 - val_accuracy: 0.9426\n",
      "Epoch 36/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.2716 - accuracy: 0.9095 - val_loss: 0.2268 - val_accuracy: 0.9370\n",
      "Epoch 37/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2650 - accuracy: 0.9146 - val_loss: 0.2083 - val_accuracy: 0.9453\n",
      "Epoch 38/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2643 - accuracy: 0.9137 - val_loss: 0.2172 - val_accuracy: 0.9481\n",
      "Epoch 39/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2628 - accuracy: 0.9142 - val_loss: 0.2110 - val_accuracy: 0.9462\n",
      "Epoch 40/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.2529 - accuracy: 0.9146 - val_loss: 0.2118 - val_accuracy: 0.9444\n",
      "Epoch 41/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.2604 - accuracy: 0.9143 - val_loss: 0.2060 - val_accuracy: 0.9426\n",
      "Epoch 42/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2488 - accuracy: 0.9182 - val_loss: 0.2047 - val_accuracy: 0.9398\n",
      "Epoch 43/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2607 - accuracy: 0.9132 - val_loss: 0.2256 - val_accuracy: 0.9449\n",
      "Epoch 44/100\n",
      "102/102 [==============================] - 5s 48ms/step - loss: 0.2567 - accuracy: 0.9179 - val_loss: 0.2043 - val_accuracy: 0.9435\n",
      "Epoch 45/100\n",
      "102/102 [==============================] - 5s 48ms/step - loss: 0.2556 - accuracy: 0.9168 - val_loss: 0.2072 - val_accuracy: 0.9462\n",
      "Epoch 46/100\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 0.2478 - accuracy: 0.9214 - val_loss: 0.1998 - val_accuracy: 0.9426\n",
      "Epoch 47/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2438 - accuracy: 0.9211 - val_loss: 0.2054 - val_accuracy: 0.9444\n",
      "Epoch 48/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2485 - accuracy: 0.9192 - val_loss: 0.2055 - val_accuracy: 0.9476\n",
      "Epoch 49/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2421 - accuracy: 0.9221 - val_loss: 0.2075 - val_accuracy: 0.9499\n",
      "Epoch 50/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2498 - accuracy: 0.9172 - val_loss: 0.2115 - val_accuracy: 0.9458\n",
      "Epoch 51/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.2408 - accuracy: 0.9229 - val_loss: 0.2022 - val_accuracy: 0.9490\n",
      "Epoch 52/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.2426 - accuracy: 0.9203 - val_loss: 0.2006 - val_accuracy: 0.9472\n",
      "Epoch 53/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2498 - accuracy: 0.9176 - val_loss: 0.2072 - val_accuracy: 0.9494\n",
      "Epoch 54/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2332 - accuracy: 0.9242 - val_loss: 0.1987 - val_accuracy: 0.9485\n",
      "Epoch 55/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.2427 - accuracy: 0.9210 - val_loss: 0.2040 - val_accuracy: 0.9499\n",
      "Epoch 56/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2308 - accuracy: 0.9273 - val_loss: 0.2064 - val_accuracy: 0.9504\n",
      "Epoch 57/100\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 0.2251 - accuracy: 0.9284 - val_loss: 0.2065 - val_accuracy: 0.9504\n",
      "Epoch 58/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2367 - accuracy: 0.9260 - val_loss: 0.1961 - val_accuracy: 0.9453\n",
      "Epoch 59/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2337 - accuracy: 0.9260 - val_loss: 0.1936 - val_accuracy: 0.9508\n",
      "Epoch 60/100\n",
      "102/102 [==============================] - 4s 40ms/step - loss: 0.2283 - accuracy: 0.9276 - val_loss: 0.1890 - val_accuracy: 0.9513\n",
      "Epoch 61/100\n",
      "102/102 [==============================] - 4s 39ms/step - loss: 0.2299 - accuracy: 0.9250 - val_loss: 0.1845 - val_accuracy: 0.9490\n",
      "Epoch 62/100\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 0.2271 - accuracy: 0.9287 - val_loss: 0.1990 - val_accuracy: 0.9494\n",
      "Epoch 63/100\n",
      "102/102 [==============================] - 5s 52ms/step - loss: 0.2366 - accuracy: 0.9255 - val_loss: 0.1910 - val_accuracy: 0.9481\n",
      "Epoch 64/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2251 - accuracy: 0.9269 - val_loss: 0.1901 - val_accuracy: 0.9481\n",
      "Epoch 65/100\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 0.2280 - accuracy: 0.9271 - val_loss: 0.1952 - val_accuracy: 0.9508\n",
      "Epoch 66/100\n",
      "102/102 [==============================] - 5s 48ms/step - loss: 0.2269 - accuracy: 0.9282 - val_loss: 0.1819 - val_accuracy: 0.9467\n",
      "Epoch 67/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.2259 - accuracy: 0.9272 - val_loss: 0.1907 - val_accuracy: 0.9536\n",
      "Epoch 68/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.2192 - accuracy: 0.9312 - val_loss: 0.1855 - val_accuracy: 0.9563\n",
      "Epoch 69/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2247 - accuracy: 0.9302 - val_loss: 0.2083 - val_accuracy: 0.9504\n",
      "Epoch 70/100\n",
      "102/102 [==============================] - 4s 40ms/step - loss: 0.2164 - accuracy: 0.9286 - val_loss: 0.1993 - val_accuracy: 0.9517\n",
      "Epoch 71/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.2188 - accuracy: 0.9309 - val_loss: 0.1886 - val_accuracy: 0.9485\n",
      "Epoch 72/100\n",
      "102/102 [==============================] - 4s 44ms/step - loss: 0.2154 - accuracy: 0.9305 - val_loss: 0.1879 - val_accuracy: 0.9504\n",
      "Epoch 73/100\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 0.2081 - accuracy: 0.9303 - val_loss: 0.2010 - val_accuracy: 0.9485\n",
      "Epoch 74/100\n",
      "102/102 [==============================] - 5s 50ms/step - loss: 0.2141 - accuracy: 0.9295 - val_loss: 0.2024 - val_accuracy: 0.9513\n",
      "Epoch 75/100\n",
      "102/102 [==============================] - 5s 51ms/step - loss: 0.2160 - accuracy: 0.9278 - val_loss: 0.1936 - val_accuracy: 0.9517\n",
      "Epoch 76/100\n",
      "102/102 [==============================] - 6s 54ms/step - loss: 0.2071 - accuracy: 0.9350 - val_loss: 0.1752 - val_accuracy: 0.9531\n",
      "Epoch 77/100\n",
      "102/102 [==============================] - 6s 54ms/step - loss: 0.2170 - accuracy: 0.9310 - val_loss: 0.1897 - val_accuracy: 0.9550\n",
      "Epoch 78/100\n",
      "102/102 [==============================] - 5s 50ms/step - loss: 0.2074 - accuracy: 0.9315 - val_loss: 0.1831 - val_accuracy: 0.9458\n",
      "Epoch 79/100\n",
      "102/102 [==============================] - 7s 65ms/step - loss: 0.2078 - accuracy: 0.9323 - val_loss: 0.1903 - val_accuracy: 0.9559\n",
      "Epoch 80/100\n",
      "102/102 [==============================] - 5s 49ms/step - loss: 0.2031 - accuracy: 0.9340 - val_loss: 0.1889 - val_accuracy: 0.9545\n",
      "Epoch 81/100\n",
      "102/102 [==============================] - 5s 46ms/step - loss: 0.2009 - accuracy: 0.9346 - val_loss: 0.2077 - val_accuracy: 0.9508\n",
      "Epoch 82/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2049 - accuracy: 0.9332 - val_loss: 0.2007 - val_accuracy: 0.9531\n",
      "Epoch 83/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.2105 - accuracy: 0.9314 - val_loss: 0.1821 - val_accuracy: 0.9545\n",
      "Epoch 84/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.2056 - accuracy: 0.9344 - val_loss: 0.1932 - val_accuracy: 0.9536\n",
      "Epoch 85/100\n",
      "102/102 [==============================] - 5s 51ms/step - loss: 0.1993 - accuracy: 0.9393 - val_loss: 0.1926 - val_accuracy: 0.9522\n",
      "Epoch 86/100\n",
      "102/102 [==============================] - 5s 53ms/step - loss: 0.2003 - accuracy: 0.9367 - val_loss: 0.1815 - val_accuracy: 0.9536\n",
      "Epoch 87/100\n",
      "102/102 [==============================] - 6s 58ms/step - loss: 0.2021 - accuracy: 0.9360 - val_loss: 0.1802 - val_accuracy: 0.9504\n",
      "Epoch 88/100\n",
      "102/102 [==============================] - 6s 61ms/step - loss: 0.1985 - accuracy: 0.9337 - val_loss: 0.1879 - val_accuracy: 0.9481\n",
      "Epoch 89/100\n",
      "102/102 [==============================] - 5s 49ms/step - loss: 0.2011 - accuracy: 0.9393 - val_loss: 0.1880 - val_accuracy: 0.9573\n",
      "Epoch 90/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.1997 - accuracy: 0.9356 - val_loss: 0.2042 - val_accuracy: 0.9522\n",
      "Epoch 91/100\n",
      "102/102 [==============================] - 5s 45ms/step - loss: 0.1953 - accuracy: 0.9400 - val_loss: 0.1890 - val_accuracy: 0.9540\n",
      "Epoch 92/100\n",
      "102/102 [==============================] - 5s 44ms/step - loss: 0.1913 - accuracy: 0.9362 - val_loss: 0.1874 - val_accuracy: 0.9550\n",
      "Epoch 93/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.1977 - accuracy: 0.9359 - val_loss: 0.1731 - val_accuracy: 0.9504\n",
      "Epoch 94/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.1980 - accuracy: 0.9366 - val_loss: 0.1833 - val_accuracy: 0.9462\n",
      "Epoch 95/100\n",
      "102/102 [==============================] - 5s 47ms/step - loss: 0.1951 - accuracy: 0.9363 - val_loss: 0.1854 - val_accuracy: 0.9508\n",
      "Epoch 96/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.1968 - accuracy: 0.9335 - val_loss: 0.1805 - val_accuracy: 0.9563\n",
      "Epoch 97/100\n",
      "102/102 [==============================] - 4s 43ms/step - loss: 0.1842 - accuracy: 0.9421 - val_loss: 0.1864 - val_accuracy: 0.9563\n",
      "Epoch 98/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.1969 - accuracy: 0.9358 - val_loss: 0.2150 - val_accuracy: 0.9527\n",
      "Epoch 99/100\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.1864 - accuracy: 0.9396 - val_loss: 0.1925 - val_accuracy: 0.9508\n",
      "Epoch 100/100\n",
      "102/102 [==============================] - 4s 42ms/step - loss: 0.1870 - accuracy: 0.9402 - val_loss: 0.1977 - val_accuracy: 0.9485\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "                                    # changed model name\n",
    "model_checkpoint = ModelCheckpoint(\"models/rhyme_model_2.hdf5\",monitor=\"val_loss\")\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "csv_logger = CSVLogger('training2.log')\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]],\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint, terminate_on_nan, csv_logger],\n",
    "    validation_data=([X_val[:, 0], X_val[:, 1]], y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_567164/3206255790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/rhyme_model_2.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model(\"models/rhyme_model_2.hdf5\")\n",
    "\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "y_pred = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      4352\n",
      "           1       0.91      0.95      0.93      2176\n",
      "\n",
      "    accuracy                           0.95      6528\n",
      "   macro avg       0.94      0.95      0.95      6528\n",
      "weighted avg       0.95      0.95      0.95      6528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric 1: Du sitter og ser så dum ut\n",
      "Lyric 2: Idiot, din jævla stut\n",
      "Rhyme(0.8702999949455261)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Er du sikker på at dette er med vilje?\n",
      "Lyric 2: Hjertet ditt smaker vanilje\n",
      "Rhyme(0.8730999827384949)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Dette vokser\n",
      "Lyric 2: Satans underbukser\n",
      "Rhyme(0.8730999827384949)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Hva er meninga med livet?\n",
      "Lyric 2: Ikke vet jeg.\n",
      "Non-rhyme(0.0)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Dette sier Gud\n",
      "Lyric 2: Ve deg! lille menneske\n",
      "Non-rhyme(0.0)\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "   [\"Du sitter og ser så dum ut\", \"Idiot, din jævla stut\"], \n",
    "   [\"Er du sikker på at dette er med vilje?\", \"Hjertet ditt smaker vanilje\"], \n",
    "   [\"Dette vokser\", \"Satans underbukser\"],\n",
    "   [\"Hva er meninga med livet?\", \"Ikke vet jeg.\"], \n",
    "   [\"Dette sier Gud\", \"Ve deg! lille menneske\"], \n",
    "]\n",
    "\n",
    "sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "for i in range(len(samples)):\n",
    "    print(f\"Lyric 1: {samples[i][0]}\")\n",
    "    print(f\"Lyric 2: {samples[i][1]}\")\n",
    "    print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric 1: Kan du ikke se det?\n",
      "Lyric 2: Deg skal jeg lede\n",
      "Non-rhyme(0.2529999911785126)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Kaker av alle slag\n",
      "Lyric 2: Her henger Norges flagg\n",
      "Rhyme(0.8363999724388123)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Jeg har ikke tid\n",
      "Lyric 2: Til dette svineri\n",
      "Rhyme(0.8282999992370605)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Hva har du sagt\n",
      "Lyric 2: Kaken er bakt\n",
      "Rhyme(0.8730999827384949)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Barna er lagt\n",
      "Lyric 2: Kaken er laget\n",
      "Non-rhyme(0.0)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Er du sikker på at dette er med vilje?\n",
      "Lyric 2: Hjertet ditt smaker vanilje\n",
      "Rhyme(0.8730999827384949)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Dette vokser\n",
      "Lyric 2: Satans underbukser\n",
      "Rhyme(0.8730999827384949)\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "   [\"Kan du ikke se det?\", \"Deg skal jeg lede\"], \n",
    "   [\"Kaker av alle slag\", \"Her henger Norges flagg\"], \n",
    "   [\"Jeg har ikke tid\", \"Til dette svineri\"],\n",
    "   [\"Hva har du sagt\", \"Kaken er bakt\"], \n",
    "   [\"Barna er lagt\", \"Kaken er laget\"],\n",
    "    [\"Er du sikker på at dette er med vilje?\", \"Hjertet ditt smaker vanilje\"], \n",
    "   [\"Dette vokser\", \"Satans underbukser\"],\n",
    "]\n",
    "\n",
    "sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "for i in range(len(samples)):\n",
    "    print(f\"Lyric 1: {samples[i][0]}\")\n",
    "    print(f\"Lyric 2: {samples[i][1]}\")\n",
    "    print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "    print(\"---------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
