{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/minoguep/rhyme_detection and https://paulminogue.com/index.php/2021/02/14/using-a-siamese-neural-network-to-create-a-simple-rhyme-detector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 00:03:28.903884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-03 00:03:28.903924: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Subtract\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TerminateOnNaN, CSVLogger, EarlyStopping\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "MAX_LEN = 64\n",
    "SEED = 420\n",
    "# sets random, np.random and tf.random seed\n",
    "tf.keras.utils.set_random_seed(\n",
    "    SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create dataset\n",
    "We want equally many positive and negative samples of rhyme pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_a</th>\n",
       "      <th>word_b</th>\n",
       "      <th>rhyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fantasier</td>\n",
       "      <td>melodier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fullkommenhet</td>\n",
       "      <td>atlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heder</td>\n",
       "      <td>der</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hugger</td>\n",
       "      <td>vugger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pergamenter</td>\n",
       "      <td>argumenter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>griper</td>\n",
       "      <td>lenger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>ett</td>\n",
       "      <td>kast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>ord</td>\n",
       "      <td>art</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27218</th>\n",
       "      <td>ikke</td>\n",
       "      <td>trette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>tegn</td>\n",
       "      <td>lun</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word_a      word_b  rhyme\n",
       "1          fantasier    melodier      1\n",
       "2      fullkommenhet       atlet      1\n",
       "3              heder         der      1\n",
       "6             hugger      vugger      1\n",
       "20       pergamenter  argumenter      1\n",
       "...              ...         ...    ...\n",
       "8403          griper      lenger      0\n",
       "8178             ett        kast      0\n",
       "2303             ord         art      0\n",
       "27218           ikke      trette      0\n",
       "3459            tegn         lun      0\n",
       "\n",
       "[14512 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhyme_pairs = pd.read_csv(\"rhyme_word_pairs.tsv\", sep=\"\\t\",)\n",
    "pos = rhyme_pairs.loc[rhyme_pairs[\"rhyme\"]==1].copy()\n",
    "neg = rhyme_pairs.loc[rhyme_pairs[\"rhyme\"]==0].copy()\n",
    "neg = neg.sample(n=len(pos), random_state=SEED)\n",
    "df = pd.concat([pos, neg])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"tita_rhymes_poems.tsv\", sep=\"\\t\")\n",
    "\n",
    "all_text = \"\"\n",
    "for e in all_data.stanza:\n",
    "    all_text += e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create model\n",
    "Copy paste from Pauls notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inputs(phrase_a, phrase_b, tokenizer):\n",
    "    tokenized_phrases = tokenizer.texts_to_sequences([phrase_a, phrase_b])\n",
    "\n",
    "    # now loop through inputs and pad or reduce size if required\n",
    "    tokenized_phrases_for_output = []\n",
    "    for phrase in tokenized_phrases:\n",
    "        if len(phrase) < MAX_LEN:\n",
    "            length_to_pad = MAX_LEN - len(phrase)\n",
    "            phrase_for_output = ([0] * length_to_pad) + phrase\n",
    "        elif len(phrase) > MAX_LEN:\n",
    "            phrase_for_output = phrase[-MAX_LEN:]\n",
    "        else:\n",
    "            phrase_for_output = phrase\n",
    "        tokenized_phrases_for_output.append(phrase_for_output)\n",
    "\n",
    "    return tf.constant(tokenized_phrases_for_output, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2101d9b6424e4702af753ec8464459e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 00:03:31.671781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-03 00:03:31.671809: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-03 00:03:31.671829: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tita-laptop): /proc/driver/nvidia/version does not exist\n",
      "2022-04-03 00:03:31.672794: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=True, lower=True)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "df['word_tokens'] = df.progress_apply(\n",
    "    lambda row: tokenize_inputs(row['word_a'], row['word_b'], tokenizer), axis=1\n",
    ")\n",
    "\n",
    "tokenizer_config = tokenizer.to_json()\n",
    "\n",
    "with open('tokenizer_config.json', 'w') as f:\n",
    "    f.write(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  \n",
    "    word_a_input_tokens = Input(\n",
    "      shape=(MAX_LEN, 1), name='word_a_input_tokens'\n",
    "      )\n",
    "    word_b_input_tokens = Input(\n",
    "      shape=(MAX_LEN, 1), name='word_b_input_tokens'\n",
    "      )\n",
    "    \n",
    "    # This is the siamese portion of the model \n",
    "    common_lstm = LSTM(64, return_sequences=False, activation=\"relu\", name=\"common_lstm_layer\")\n",
    "\n",
    "    word_a_lstm_output = common_lstm(word_a_input_tokens)\n",
    "    word_b_lstm_output = common_lstm(word_b_input_tokens)\n",
    "\n",
    "    #concatenate_lstm_outputs\n",
    "    concat_layer = Subtract(name=\"concatenate_lstm_outputs\")(\n",
    "      [word_a_lstm_output, word_b_lstm_output]\n",
    "      )\n",
    "    \n",
    "    # dense layers before final classification\n",
    "    dense_layers = Dense(64, activation=\"relu\", name=\"first_dense_layer\")(concat_layer)\n",
    "    dense_layers = Dropout(0.5)(dense_layers)\n",
    "\n",
    "    dense_layers = Dense(32, activation=\"relu\", name=\"second_dense_layer\")(dense_layers)\n",
    "    dense_layers = Dropout(0.5)(dense_layers)\n",
    "\n",
    "    dense_layers = Dense(8, activation=\"relu\", name=\"third_dense_layer\")(dense_layers)\n",
    "    dense_layers = Dropout(0.5)(dense_layers)\n",
    "\n",
    "    classification_layer = Dense(1, activation=\"sigmoid\", name=\"classification_layer\")(dense_layers)\n",
    "    \n",
    "    model = Model(\n",
    "      inputs=[word_a_input_tokens, word_b_input_tokens], \n",
    "      outputs = classification_layer\n",
    "      )\n",
    "\n",
    "    model.compile(\n",
    "      loss=\"binary_crossentropy\",\n",
    "      metrics=[\"accuracy\"],\n",
    "      optimizer=\"Adam\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indexes, X_test_indexes, y_train, y_test = train_test_split(\n",
    "    list(df.index), list(df['rhyme']), stratify=df['rhyme'], \n",
    "    test_size=0.4, random_state=SEED\n",
    "    )\n",
    "\n",
    "X_test_indexes, X_val_indexes, y_test, y_val = train_test_split(\n",
    "    X_test_indexes, y_test, stratify=y_test, \n",
    "    test_size=0.25, random_state=SEED\n",
    "    )\n",
    "\n",
    "X_train = tf.convert_to_tensor(list(df.loc[X_train_indexes][\"word_tokens\"]))\n",
    "X_val = tf.convert_to_tensor(list(df.loc[X_val_indexes][\"word_tokens\"]))\n",
    "X_test = tf.convert_to_tensor(list(df.loc[X_test_indexes][\"word_tokens\"]))\n",
    "\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Data set size: Full set: 14512\n",
      "    Train: 8707\n",
      "    Validation: 1452\n",
      "    Test: 4353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "    Data set size: Full set: {len(df)}\n",
    "    Train: {len(X_train)}\n",
    "    Validation: {len(X_val)}\n",
    "    Test: {len(X_test)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rhyme_model1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Uncomment to train model \n",
    "\n",
    "# model = create_model()\n",
    "\n",
    "# model_checkpoint = ModelCheckpoint(f\"models/{model_name}.hdf5\",monitor=\"val_loss\")\n",
    "# terminate_on_nan = TerminateOnNaN()\n",
    "# csv_logger = CSVLogger(f'logs/training_{model_name}.log')\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train[:, 0], X_train[:, 1]],\n",
    "#     y_train,\n",
    "#     batch_size=128,\n",
    "#     epochs=100,\n",
    "#     callbacks=[model_checkpoint, terminate_on_nan, csv_logger, early_stop],\n",
    "#     validation_data=([X_val[:, 0], X_val[:, 1]], y_val)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      2176\n",
      "           1       0.92      0.96      0.94      2177\n",
      "\n",
      "    accuracy                           0.94      4353\n",
      "   macro avg       0.94      0.94      0.94      4353\n",
      "weighted avg       0.94      0.94      0.94      4353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model(f\"models/{model_name}.hdf5\")\n",
    "\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "y_pred = y_pred > 0.5\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 64) (7, 64)\n",
      "Sentence 1: Kan du ikke se det\n",
      "Sentence 2: Deg skal jeg lede\n",
      "Rhyme(0.6941999793052673)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Kaker av alle slag\n",
      "Sentence 2: Her henger Norges flagg\n",
      "Rhyme(0.8702999949455261)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Jeg har ikke tid\n",
      "Sentence 2: Til dette svineri\n",
      "Rhyme(0.7032999992370605)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Hva har du sagt\n",
      "Sentence 2: Kaken er bakt\n",
      "Rhyme(0.9254999756813049)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Barna er lagt\n",
      "Sentence 2: Kaken er laget\n",
      "Non-rhyme(0.000699999975040555)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Gjorde du det med vilje\n",
      "Sentence 2: Kaken smaker vanilje\n",
      "Rhyme(0.9366000294685364)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Dette vokser\n",
      "Sentence 2: Satans underbukser\n",
      "Rhyme(0.9340999722480774)\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    [\"Kan du ikke se det\", \"Deg skal jeg lede\"], \n",
    "    [\"Kaker av alle slag\", \"Her henger Norges flagg\"], \n",
    "    [\"Jeg har ikke tid\", \"Til dette svineri\"],\n",
    "    [\"Hva har du sagt\", \"Kaken er bakt\"], \n",
    "    [\"Barna er lagt\", \"Kaken er laget\"],\n",
    "    [\"Gjorde du det med vilje\", \"Kaken smaker vanilje\"], \n",
    "    [\"Dette vokser\", \"Satans underbukser\"],\n",
    "]\n",
    "\n",
    "sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "\n",
    "print(sample_tokens[:, 0].shape, sample_tokens[:, 1].shape)\n",
    "\n",
    "sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "for i in range(len(samples)):\n",
    "    print(f\"Sentence 1: {samples[i][0]}\")\n",
    "    print(f\"Sentence 2: {samples[i][1]}\")\n",
    "    print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try again with mirrored examples included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mirrored_df(df):\n",
    "    mirror = pd.DataFrame({\"word_a\": df[\"word_b\"], \n",
    "                           \"word_b\": df[\"word_a\"], \n",
    "                           \"rhyme\": df[\"rhyme\"], \n",
    "                           \"word_tokens\":[(t[1], t[0]) for t in df[\"word_tokens\"]]})\n",
    "    return pd.concat((df, mirror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8707, 1452)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.loc[X_train_indexes]\n",
    "dev_df = df.loc[X_val_indexes]\n",
    "len(train_df), len(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_a</th>\n",
       "      <th>word_b</th>\n",
       "      <th>rhyme</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8032</th>\n",
       "      <td>venn</td>\n",
       "      <td>formår</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24484</th>\n",
       "      <td>ville</td>\n",
       "      <td>Røst</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>elven</td>\n",
       "      <td>støv</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>båsen</td>\n",
       "      <td>tomme</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>slaver</td>\n",
       "      <td>tror</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>støv</td>\n",
       "      <td>ung</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19712</th>\n",
       "      <td>dødningdrakt</td>\n",
       "      <td>prakt</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19219</th>\n",
       "      <td>årevis</td>\n",
       "      <td>flis</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22260</th>\n",
       "      <td>bue</td>\n",
       "      <td>strand</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9030</th>\n",
       "      <td>velde</td>\n",
       "      <td>nelde</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17414 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_a  word_b  rhyme  \\\n",
       "8032           venn  formår      0   \n",
       "24484         ville    Røst      0   \n",
       "4255          elven    støv      0   \n",
       "24330         båsen   tomme      0   \n",
       "7665         slaver    tror      0   \n",
       "...             ...     ...    ...   \n",
       "14350          støv     ung      0   \n",
       "19712  dødningdrakt   prakt      1   \n",
       "19219        årevis    flis      1   \n",
       "22260           bue  strand      0   \n",
       "9030          velde   nelde      1   \n",
       "\n",
       "                                             word_tokens  \n",
       "8032   ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "24484  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "4255   ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "24330  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "7665   ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "...                                                  ...  \n",
       "14350  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "19712  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "19219  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "22260  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "9030   ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "\n",
       "[17414 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_train = get_mirrored_df(train_df)\n",
    "double_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_a</th>\n",
       "      <th>word_b</th>\n",
       "      <th>rhyme</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25195</th>\n",
       "      <td>leve</td>\n",
       "      <td>vann</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>jord</td>\n",
       "      <td>ror</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12891</th>\n",
       "      <td>før</td>\n",
       "      <td>snakk</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23019</th>\n",
       "      <td>fargespill</td>\n",
       "      <td>mild</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26071</th>\n",
       "      <td>grisen</td>\n",
       "      <td>ekspeditrisen</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17849</th>\n",
       "      <td>før</td>\n",
       "      <td>tørr</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11377</th>\n",
       "      <td>hånd</td>\n",
       "      <td>ild</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16025</th>\n",
       "      <td>fortell</td>\n",
       "      <td>herskerinnen</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28151</th>\n",
       "      <td>skatt</td>\n",
       "      <td>befale</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20806</th>\n",
       "      <td>frokostbordet</td>\n",
       "      <td>ordet</td>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float64), tf....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2904 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word_a         word_b  rhyme  \\\n",
       "25195           leve           vann      0   \n",
       "8166            jord            ror      1   \n",
       "12891            før          snakk      0   \n",
       "23019     fargespill           mild      1   \n",
       "26071         grisen  ekspeditrisen      1   \n",
       "...              ...            ...    ...   \n",
       "17849            før           tørr      1   \n",
       "11377           hånd            ild      0   \n",
       "16025        fortell   herskerinnen      0   \n",
       "28151          skatt         befale      0   \n",
       "20806  frokostbordet          ordet      1   \n",
       "\n",
       "                                             word_tokens  \n",
       "25195  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "8166   ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "12891  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "23019  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "26071  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "...                                                  ...  \n",
       "17849  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "11377  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "16025  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "28151  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "20806  ((tf.Tensor(0.0, shape=(), dtype=float64), tf....  \n",
       "\n",
       "[2904 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_dev = get_mirrored_df(dev_df)\n",
    "double_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(list(double_train[\"word_tokens\"]))\n",
    "X_val = tf.convert_to_tensor(list(double_dev[\"word_tokens\"]))\n",
    "\n",
    "y_train = tf.convert_to_tensor(list(double_train[\"rhyme\"]))\n",
    "y_val = tf.convert_to_tensor(list(double_dev[\"rhyme\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rhyme_model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to train model \n",
    "# model = create_model()\n",
    "\n",
    "# model_checkpoint = ModelCheckpoint(f\"models/{model_name}.hdf5\",monitor=\"val_loss\")\n",
    "# terminate_on_nan = TerminateOnNaN()\n",
    "# csv_logger = CSVLogger(f'logs/training_{model_name}.log')\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train[:, 0], X_train[:, 1]],\n",
    "#     y_train,\n",
    "#     batch_size=128,\n",
    "#     epochs=100,\n",
    "#     callbacks=[model_checkpoint, terminate_on_nan, csv_logger, early_stop],\n",
    "#     validation_data=([X_val[:, 0], X_val[:, 1]], y_val)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      2176\n",
      "           1       0.97      0.93      0.95      2177\n",
      "\n",
      "    accuracy                           0.95      4353\n",
      "   macro avg       0.95      0.95      0.95      4353\n",
      "weighted avg       0.95      0.95      0.95      4353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model(f\"models/{model_name}.hdf5\")\n",
    "\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "y_pred = y_pred > 0.5\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 64) (7, 64)\n",
      "Sentence 1: Kan du ikke se det\n",
      "Sentence 2: Deg skal jeg lede\n",
      "Non-rhyme(0.14920000731945038)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Kaker av alle slag\n",
      "Sentence 2: Her henger Norges flagg\n",
      "Rhyme(0.9937000274658203)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Jeg har ikke tid\n",
      "Sentence 2: Til dette svineri\n",
      "Non-rhyme(0.25589999556541443)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Hva har du sagt\n",
      "Sentence 2: Kaken er bakt\n",
      "Rhyme(0.9451000094413757)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Barna er lagt\n",
      "Sentence 2: Kaken er laget\n",
      "Non-rhyme(0.01209999993443489)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Gjorde du det med vilje\n",
      "Sentence 2: Kaken smaker vanilje\n",
      "Rhyme(0.9998000264167786)\n",
      "---------------\n",
      "\n",
      "Sentence 1: Dette vokser\n",
      "Sentence 2: Satans underbukser\n",
      "Rhyme(0.9998000264167786)\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    [\"Kan du ikke se det\", \"Deg skal jeg lede\"], \n",
    "    [\"Kaker av alle slag\", \"Her henger Norges flagg\"], \n",
    "    [\"Jeg har ikke tid\", \"Til dette svineri\"],\n",
    "    [\"Hva har du sagt\", \"Kaken er bakt\"], \n",
    "    [\"Barna er lagt\", \"Kaken er laget\"],\n",
    "    [\"Gjorde du det med vilje\", \"Kaken smaker vanilje\"], \n",
    "    [\"Dette vokser\", \"Satans underbukser\"],\n",
    "]\n",
    "\n",
    "sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "\n",
    "print(sample_tokens[:, 0].shape, sample_tokens[:, 1].shape)\n",
    "\n",
    "sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "for i in range(len(samples)):\n",
    "    print(f\"Sentence 1: {samples[i][0]}\")\n",
    "    print(f\"Sentence 2: {samples[i][1]}\")\n",
    "    print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try again with different 2:3 positive to negative ratio\n",
    "Use same test and dev sets, and same tokenizer. Only expand training set.  \n",
    "Train set is already 50/50 positive and negative. We want to make it 40/60 --> increase negative examples by half of what we already have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_neg = len(neg) // 2\n",
    "new_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = rhyme_pairs.loc[rhyme_pairs[\"rhyme\"]==0].copy()\n",
    "# use same seed, and extract all the pairs already used + the ones we need \n",
    "negative2 = neg.sample(n=len(pos) + new_neg, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_a</th>\n",
       "      <th>word_b</th>\n",
       "      <th>rhyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13954</th>\n",
       "      <td>fri</td>\n",
       "      <td>mere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>tinder</td>\n",
       "      <td>bedøvet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8417</th>\n",
       "      <td>ny</td>\n",
       "      <td>vann</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16469</th>\n",
       "      <td>gret</td>\n",
       "      <td>visste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15765</th>\n",
       "      <td>Ola</td>\n",
       "      <td>laks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>geseller</td>\n",
       "      <td>skam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19487</th>\n",
       "      <td>like</td>\n",
       "      <td>merke</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>sinn</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>sinn</td>\n",
       "      <td>kvist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21642</th>\n",
       "      <td>nesset</td>\n",
       "      <td>gjerdestav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3628 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_a      word_b  rhyme\n",
       "13954       fri        mere      0\n",
       "3616     tinder     bedøvet      0\n",
       "8417         ny        vann      0\n",
       "16469      gret      visste      0\n",
       "15765       Ola        laks      0\n",
       "...         ...         ...    ...\n",
       "2735   geseller        skam      0\n",
       "19487      like       merke      0\n",
       "249        sinn         ham      0\n",
       "2445       sinn       kvist      0\n",
       "21642    nesset  gjerdestav      0\n",
       "\n",
       "[3628 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unused_neg = negative2[len(pos):]\n",
    "unused_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e2d32d47924de097fdf31c5240db99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13850/2379011568.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unused_neg['word_tokens'] = unused_neg.progress_apply(\n"
     ]
    }
   ],
   "source": [
    "# use same tokenizer as above\n",
    "unused_neg['word_tokens'] = unused_neg.progress_apply(\n",
    "    lambda row: tokenize_inputs(row['word_a'], row['word_b'], tokenizer), axis=1\n",
    ")\n",
    "\n",
    "unused_neg = get_mirrored_df(unused_neg)\n",
    "\n",
    "X_train_new = tf.convert_to_tensor(list(X_train) + list(unused_neg[\"word_tokens\"]))\n",
    "y_train_new = tf.convert_to_tensor(list(y_train) + list(unused_neg[\"rhyme\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([24670, 2, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rhyme_model_23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to train model\n",
    "# model = create_model()\n",
    "\n",
    "# model_checkpoint = ModelCheckpoint(f\"models/{model_name}.hdf5\",monitor=\"val_loss\")\n",
    "# terminate_on_nan = TerminateOnNaN()\n",
    "# csv_logger = CSVLogger(f'logs/training_{model_name}.log')\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train_new[:, 0], X_train_new[:, 1]],\n",
    "#     y_train_new,\n",
    "#     batch_size=128,\n",
    "#     epochs=100,\n",
    "#     callbacks=[model_checkpoint, terminate_on_nan, csv_logger, early_stop],\n",
    "#     validation_data=([X_val[:, 0], X_val[:, 1]], y_val)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      2176\n",
      "           1       0.97      0.95      0.96      2177\n",
      "\n",
      "    accuracy                           0.96      4353\n",
      "   macro avg       0.96      0.96      0.96      4353\n",
      "weighted avg       0.96      0.96      0.96      4353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model(f\"models/{model_name}.hdf5\")\n",
    "\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "y_pred = y_pred > 0.5\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric 1: Kan du ikke se det\n",
      "Lyric 2: Deg skal jeg lede\n",
      "Rhyme(0.9348999857902527)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Kaker av alle slag\n",
      "Lyric 2: Her henger Norges flagg\n",
      "Non-rhyme(0.2143000066280365)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Jeg har ikke tid\n",
      "Lyric 2: Til dette svineri\n",
      "Rhyme(0.9348999857902527)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Hva har du sagt\n",
      "Lyric 2: Kaken er bakt\n",
      "Rhyme(0.9348999857902527)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Barna er lagt\n",
      "Lyric 2: Kaken er laget\n",
      "Non-rhyme(0.0)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Gjorde du det med vilje\n",
      "Lyric 2: Kaken smaker vanilje\n",
      "Rhyme(0.9348999857902527)\n",
      "---------------\n",
      "\n",
      "Lyric 1: Dette vokser\n",
      "Lyric 2: Satans underbukser\n",
      "Rhyme(0.849399983882904)\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    [\"Kan du ikke se det\", \"Deg skal jeg lede\"], \n",
    "    [\"Kaker av alle slag\", \"Her henger Norges flagg\"], \n",
    "    [\"Jeg har ikke tid\", \"Til dette svineri\"],\n",
    "    [\"Hva har du sagt\", \"Kaken er bakt\"], \n",
    "    [\"Barna er lagt\", \"Kaken er laget\"],\n",
    "    [\"Gjorde du det med vilje\", \"Kaken smaker vanilje\"], \n",
    "    [\"Dette vokser\", \"Satans underbukser\"],\n",
    "]\n",
    "\n",
    "sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "for i in range(len(samples)):\n",
    "    print(f\"Lyric 1: {samples[i][0]}\")\n",
    "    print(f\"Lyric 2: {samples[i][1]}\")\n",
    "    print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use all rhyme pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_neg_2 = neg.sample(n=len(neg), random_state=SEED)[len(negative2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4ec4ffc9c54f45b85466c53a6b6182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([47796, 2, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use same tokenizer as above\n",
    "unused_neg_2['word_tokens'] = unused_neg_2.progress_apply(\n",
    "    lambda row: tokenize_inputs(row['word_a'], row['word_b'], tokenizer), axis=1\n",
    ")\n",
    "\n",
    "unused_neg_2 = get_mirrored_df(unused_neg_2)\n",
    "\n",
    "X_train_new = tf.convert_to_tensor(list(X_train_new) + list(unused_neg_2[\"word_tokens\"]))\n",
    "y_train_new = tf.convert_to_tensor(list(y_train_new) + list(unused_neg_2[\"rhyme\"]))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rhyme_model_all_pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "374/374 [==============================] - 17s 40ms/step - loss: 0.3816 - accuracy: 0.8105 - val_loss: 0.5525 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "374/374 [==============================] - 14s 39ms/step - loss: 0.2974 - accuracy: 0.8657 - val_loss: 0.5257 - val_accuracy: 0.8592\n",
      "Epoch 3/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.2740 - accuracy: 0.9289 - val_loss: 0.4851 - val_accuracy: 0.8674\n",
      "Epoch 4/100\n",
      "374/374 [==============================] - 17s 44ms/step - loss: 0.2543 - accuracy: 0.9320 - val_loss: 0.4277 - val_accuracy: 0.8922\n",
      "Epoch 5/100\n",
      "374/374 [==============================] - 16s 43ms/step - loss: 0.2384 - accuracy: 0.9321 - val_loss: 0.3989 - val_accuracy: 0.8988\n",
      "Epoch 6/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.2307 - accuracy: 0.9332 - val_loss: 0.3711 - val_accuracy: 0.9170\n",
      "Epoch 7/100\n",
      "374/374 [==============================] - 17s 46ms/step - loss: 0.2228 - accuracy: 0.9329 - val_loss: 0.3498 - val_accuracy: 0.9136\n",
      "Epoch 8/100\n",
      "374/374 [==============================] - 17s 46ms/step - loss: 0.2123 - accuracy: 0.9338 - val_loss: 0.3711 - val_accuracy: 0.9094\n",
      "Epoch 9/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.2074 - accuracy: 0.9364 - val_loss: 0.3230 - val_accuracy: 0.9370\n",
      "Epoch 10/100\n",
      "374/374 [==============================] - 15s 39ms/step - loss: 0.2030 - accuracy: 0.9374 - val_loss: 0.3552 - val_accuracy: 0.9177\n",
      "Epoch 11/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.2015 - accuracy: 0.9380 - val_loss: 0.3593 - val_accuracy: 0.9180\n",
      "Epoch 12/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.1993 - accuracy: 0.9391 - val_loss: 0.3140 - val_accuracy: 0.9353\n",
      "Epoch 13/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.1944 - accuracy: 0.9395 - val_loss: 0.2974 - val_accuracy: 0.9366\n",
      "Epoch 14/100\n",
      "374/374 [==============================] - 16s 41ms/step - loss: 0.1900 - accuracy: 0.9411 - val_loss: 0.3137 - val_accuracy: 0.9332\n",
      "Epoch 15/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1957 - accuracy: 0.9390 - val_loss: 0.3019 - val_accuracy: 0.9421\n",
      "Epoch 16/100\n",
      "374/374 [==============================] - 15s 39ms/step - loss: 0.1882 - accuracy: 0.9432 - val_loss: 0.3412 - val_accuracy: 0.9325\n",
      "Epoch 17/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.1783 - accuracy: 0.9467 - val_loss: 0.2943 - val_accuracy: 0.9411\n",
      "Epoch 18/100\n",
      "374/374 [==============================] - 19s 50ms/step - loss: 0.1735 - accuracy: 0.9487 - val_loss: 0.2854 - val_accuracy: 0.9432\n",
      "Epoch 19/100\n",
      "374/374 [==============================] - 18s 48ms/step - loss: 0.1686 - accuracy: 0.9493 - val_loss: 0.3086 - val_accuracy: 0.9360\n",
      "Epoch 20/100\n",
      "374/374 [==============================] - 17s 45ms/step - loss: 0.1660 - accuracy: 0.9498 - val_loss: 0.2730 - val_accuracy: 0.9404\n",
      "Epoch 21/100\n",
      "374/374 [==============================] - 18s 48ms/step - loss: 0.1639 - accuracy: 0.9505 - val_loss: 0.3070 - val_accuracy: 0.9439\n",
      "Epoch 22/100\n",
      "374/374 [==============================] - 20s 53ms/step - loss: 0.1651 - accuracy: 0.9518 - val_loss: 0.2959 - val_accuracy: 0.9387\n",
      "Epoch 23/100\n",
      "374/374 [==============================] - 19s 51ms/step - loss: 0.1650 - accuracy: 0.9509 - val_loss: 0.2942 - val_accuracy: 0.9397\n",
      "Epoch 24/100\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.1612 - accuracy: 0.9531 - val_loss: 0.2860 - val_accuracy: 0.9380\n",
      "Epoch 25/100\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.1635 - accuracy: 0.9520 - val_loss: 0.2859 - val_accuracy: 0.9377\n",
      "Epoch 26/100\n",
      "374/374 [==============================] - 15s 39ms/step - loss: 0.1567 - accuracy: 0.9536 - val_loss: 0.2597 - val_accuracy: 0.9459\n",
      "Epoch 27/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1534 - accuracy: 0.9550 - val_loss: 0.3027 - val_accuracy: 0.9373\n",
      "Epoch 28/100\n",
      "374/374 [==============================] - 16s 43ms/step - loss: 0.1515 - accuracy: 0.9567 - val_loss: 0.2965 - val_accuracy: 0.9394\n",
      "Epoch 29/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1558 - accuracy: 0.9542 - val_loss: 0.2834 - val_accuracy: 0.9425\n",
      "Epoch 30/100\n",
      "374/374 [==============================] - 16s 43ms/step - loss: 0.1494 - accuracy: 0.9570 - val_loss: 0.2433 - val_accuracy: 0.9497\n",
      "Epoch 31/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1487 - accuracy: 0.9567 - val_loss: 0.3331 - val_accuracy: 0.9349\n",
      "Epoch 32/100\n",
      "374/374 [==============================] - 16s 44ms/step - loss: 0.1452 - accuracy: 0.9563 - val_loss: 0.3012 - val_accuracy: 0.9411\n",
      "Epoch 33/100\n",
      "374/374 [==============================] - 16s 43ms/step - loss: 0.1457 - accuracy: 0.9563 - val_loss: 0.2826 - val_accuracy: 0.9452\n",
      "Epoch 34/100\n",
      "374/374 [==============================] - 16s 43ms/step - loss: 0.1419 - accuracy: 0.9575 - val_loss: 0.2854 - val_accuracy: 0.9480\n",
      "Epoch 35/100\n",
      "374/374 [==============================] - 17s 45ms/step - loss: 0.1445 - accuracy: 0.9576 - val_loss: 0.2605 - val_accuracy: 0.9477\n",
      "Epoch 36/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1376 - accuracy: 0.9601 - val_loss: 0.2546 - val_accuracy: 0.9494\n",
      "Epoch 37/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.1398 - accuracy: 0.9596 - val_loss: 0.2796 - val_accuracy: 0.9463\n",
      "Epoch 38/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1368 - accuracy: 0.9580 - val_loss: 0.2802 - val_accuracy: 0.9473\n",
      "Epoch 39/100\n",
      "374/374 [==============================] - 17s 44ms/step - loss: 0.1368 - accuracy: 0.9595 - val_loss: 0.2617 - val_accuracy: 0.9532\n",
      "Epoch 40/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.1369 - accuracy: 0.9592 - val_loss: 0.2861 - val_accuracy: 0.9497\n",
      "Epoch 41/100\n",
      "374/374 [==============================] - 16s 43ms/step - loss: 0.1350 - accuracy: 0.9610 - val_loss: 0.2971 - val_accuracy: 0.9360\n",
      "Epoch 42/100\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.1359 - accuracy: 0.9599 - val_loss: 0.2964 - val_accuracy: 0.9514\n",
      "Epoch 43/100\n",
      "374/374 [==============================] - 16s 42ms/step - loss: 0.1290 - accuracy: 0.9626 - val_loss: 0.2771 - val_accuracy: 0.9490\n",
      "Epoch 44/100\n",
      "374/374 [==============================] - 15s 41ms/step - loss: 0.1288 - accuracy: 0.9619 - val_loss: 0.2661 - val_accuracy: 0.9514\n",
      "Epoch 45/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1313 - accuracy: 0.9612 - val_loss: 0.2794 - val_accuracy: 0.9539\n",
      "Epoch 46/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1297 - accuracy: 0.9629 - val_loss: 0.3003 - val_accuracy: 0.9490\n",
      "Epoch 47/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1250 - accuracy: 0.9629 - val_loss: 0.3314 - val_accuracy: 0.9449\n",
      "Epoch 48/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1214 - accuracy: 0.9659 - val_loss: 0.2656 - val_accuracy: 0.9535\n",
      "Epoch 49/100\n",
      "374/374 [==============================] - 14s 39ms/step - loss: 0.1166 - accuracy: 0.9658 - val_loss: 0.2894 - val_accuracy: 0.9442\n",
      "Epoch 50/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1169 - accuracy: 0.9670 - val_loss: 0.2567 - val_accuracy: 0.9532\n",
      "Epoch 51/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1172 - accuracy: 0.9667 - val_loss: 0.2578 - val_accuracy: 0.9511\n",
      "Epoch 52/100\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.1118 - accuracy: 0.9678 - val_loss: 0.2426 - val_accuracy: 0.9549\n",
      "Epoch 53/100\n",
      "374/374 [==============================] - 14s 39ms/step - loss: 0.1108 - accuracy: 0.9675 - val_loss: 0.3029 - val_accuracy: 0.9525\n",
      "Epoch 54/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1125 - accuracy: 0.9672 - val_loss: 0.2717 - val_accuracy: 0.9504\n",
      "Epoch 55/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1079 - accuracy: 0.9679 - val_loss: 0.3057 - val_accuracy: 0.9497\n",
      "Epoch 56/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1083 - accuracy: 0.9686 - val_loss: 0.2614 - val_accuracy: 0.9542\n",
      "Epoch 57/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1124 - accuracy: 0.9673 - val_loss: 0.3269 - val_accuracy: 0.9415\n",
      "Epoch 58/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1077 - accuracy: 0.9681 - val_loss: 0.2493 - val_accuracy: 0.9576\n",
      "Epoch 59/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1061 - accuracy: 0.9697 - val_loss: 0.2872 - val_accuracy: 0.9511\n",
      "Epoch 60/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1056 - accuracy: 0.9686 - val_loss: 0.3241 - val_accuracy: 0.9480\n",
      "Epoch 61/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1032 - accuracy: 0.9693 - val_loss: 0.2867 - val_accuracy: 0.9490\n",
      "Epoch 62/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1047 - accuracy: 0.9688 - val_loss: 0.2768 - val_accuracy: 0.9563\n",
      "Epoch 63/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1058 - accuracy: 0.9693 - val_loss: 0.2835 - val_accuracy: 0.9576\n",
      "Epoch 64/100\n",
      "374/374 [==============================] - 14s 39ms/step - loss: 0.1046 - accuracy: 0.9696 - val_loss: 0.2956 - val_accuracy: 0.9494\n",
      "Epoch 65/100\n",
      "374/374 [==============================] - 15s 39ms/step - loss: 0.0976 - accuracy: 0.9714 - val_loss: 0.2636 - val_accuracy: 0.9552\n",
      "Epoch 66/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1001 - accuracy: 0.9706 - val_loss: 0.3230 - val_accuracy: 0.9535\n",
      "Epoch 67/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1010 - accuracy: 0.9708 - val_loss: 0.3212 - val_accuracy: 0.9532\n",
      "Epoch 68/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.1018 - accuracy: 0.9700 - val_loss: 0.3205 - val_accuracy: 0.9535\n",
      "Epoch 69/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0951 - accuracy: 0.9713 - val_loss: 0.3019 - val_accuracy: 0.9518\n",
      "Epoch 70/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0945 - accuracy: 0.9714 - val_loss: 0.3184 - val_accuracy: 0.9566\n",
      "Epoch 71/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0971 - accuracy: 0.9716 - val_loss: 0.3069 - val_accuracy: 0.9525\n",
      "Epoch 72/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0962 - accuracy: 0.9720 - val_loss: 0.3657 - val_accuracy: 0.9477\n",
      "Epoch 73/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0986 - accuracy: 0.9720 - val_loss: 0.3473 - val_accuracy: 0.9504\n",
      "Epoch 74/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0930 - accuracy: 0.9722 - val_loss: 0.3008 - val_accuracy: 0.9542\n",
      "Epoch 75/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0927 - accuracy: 0.9727 - val_loss: 0.3532 - val_accuracy: 0.9508\n",
      "Epoch 76/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0913 - accuracy: 0.9731 - val_loss: 0.3108 - val_accuracy: 0.9539\n",
      "Epoch 77/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0966 - accuracy: 0.9711 - val_loss: 0.3002 - val_accuracy: 0.9528\n",
      "Epoch 78/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0919 - accuracy: 0.9733 - val_loss: 0.4177 - val_accuracy: 0.9394\n",
      "Epoch 79/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0897 - accuracy: 0.9729 - val_loss: 0.3822 - val_accuracy: 0.9508\n",
      "Epoch 80/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0909 - accuracy: 0.9725 - val_loss: 0.3440 - val_accuracy: 0.9552\n",
      "Epoch 81/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0924 - accuracy: 0.9728 - val_loss: 0.3342 - val_accuracy: 0.9539\n",
      "Epoch 82/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0868 - accuracy: 0.9741 - val_loss: 0.4185 - val_accuracy: 0.9463\n",
      "Epoch 83/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0866 - accuracy: 0.9737 - val_loss: 0.4403 - val_accuracy: 0.9452\n",
      "Epoch 84/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0900 - accuracy: 0.9738 - val_loss: 0.3448 - val_accuracy: 0.9497\n",
      "Epoch 85/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0872 - accuracy: 0.9742 - val_loss: 0.3259 - val_accuracy: 0.9552\n",
      "Epoch 86/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0874 - accuracy: 0.9732 - val_loss: 0.3172 - val_accuracy: 0.9528\n",
      "Epoch 87/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0880 - accuracy: 0.9740 - val_loss: 0.4381 - val_accuracy: 0.9480\n",
      "Epoch 88/100\n",
      "374/374 [==============================] - 14s 38ms/step - loss: 0.0888 - accuracy: 0.9726 - val_loss: 0.3993 - val_accuracy: 0.9511\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment to train model\n",
    "# model = create_model()\n",
    "\n",
    "# model_checkpoint = ModelCheckpoint(f\"models/{model_name}.hdf5\",monitor=\"val_loss\")\n",
    "# terminate_on_nan = TerminateOnNaN()\n",
    "# csv_logger = CSVLogger(f'logs/training_{model_name}.log')\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train_new[:, 0], X_train_new[:, 1]],\n",
    "#     y_train_new,\n",
    "#     batch_size=128,\n",
    "#     epochs=100,\n",
    "#     callbacks=[model_checkpoint, terminate_on_nan, csv_logger, early_stop],\n",
    "#     validation_data=([X_val[:, 0], X_val[:, 1]], y_val)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      2176\n",
      "           1       0.99      0.93      0.96      2177\n",
      "\n",
      "    accuracy                           0.96      4353\n",
      "   macro avg       0.96      0.96      0.96      4353\n",
      "weighted avg       0.96      0.96      0.96      4353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model(f\"models/{model_name}.hdf5\")\n",
    "\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "y_pred = y_pred > 0.5\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to use whole sentences instead of just word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"sentence_pair_rhyme.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [\n",
    "#     [\"Kan du ikke se det?\", \"Deg skal jeg lede\"], \n",
    "#     [\"Kaker av alle slag\", \"Her henger Norges flagg\"], \n",
    "#     [\"Jeg har ikke tid\", \"Til dette svineri\"],\n",
    "#     [\"Hva har du sagt\", \"Kaken er bakt\"], \n",
    "#     [\"Barna er lagt\", \"Kaken er laget\"],\n",
    "#     [\"Er du sikker på at dette er med vilje?\", \"Hjertet ditt smaker vanilje\"], \n",
    "#     [\"Dette vokser\", \"Satans underbukser\"],\n",
    "# ]\n",
    "\n",
    "# sample_tokens = [tokenize_inputs(lyrics[0], lyrics[1], tokenizer) for lyrics in samples]\n",
    "# sample_tokens = tf.convert_to_tensor(sample_tokens)\n",
    "# sample_pred = model.predict([sample_tokens[:, 0], sample_tokens[:, 1]])\n",
    "# predictions = [round(pred[0], 4) for pred in sample_pred]\n",
    "# for i in range(len(samples)):\n",
    "#     print(f\"Lyric 1: {samples[i][0]}\")\n",
    "#     print(f\"Lyric 2: {samples[i][1]}\")\n",
    "#     print(f\"{'Rhyme' if predictions[i] > 0.5 else 'Non-rhyme'}({predictions[i]})\")\n",
    "#     print(\"---------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
